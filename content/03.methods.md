## Methods

### Datasets

#### bioRxiv
1. Describe how bioRxiv was obtained
2. Describe metadata statistics on bioRxiv (number of preprints, number of preprints with multiple versions)

#### PubMed Central
1. Describe how PubMed central was obtained
2. Describe metadata statistics on PubMed central (number of articles, how many articles were processed

### Comparing Corpora
1. Spacy to process text via - Lemmatization, removal of stop words
2. Describe counting frequencies of each lemma
3. Describe using chi-square test
4. Describe how to calculate the likelihood and log odds ratio

### Visualizing the Preprint Landscape

#### Generate Document Representation
1. Describe how word2vec works
2. Talk about training word2vec on entire biorxiv repository
3. Discuss how to generate a document representation using word2vec model

#### Dimensionality Reduction of Document Embeddings
We used t-Distributed Stochastic Neighbor Embedding (tSNE) [@raw:tsne_Maaten] to generate a global view of the bioRxiv preprint landscape. 
This model was trained on bioRxiv document vectors using scikit-learn's [@raw:scikit-learn] implementation with a random seed of 100 and default settings for all other parameters.

We used principal component analysis (PCA) [@doi:10.1111/1467-9868.00196] to project bioRxiv document vectors into a low dimensional space.
This model was trained using scikit-learn's implementation with a random seed of 100, output of 50 principal components, and default settings for all other parameters.
For each principal component we calculated its cosine similarity with  all tokens in our word2vec model's vocab list.
We report the top 100 positive and negative scoring tokens in the form of  word clouds.

### Recommending Journals/ bioRxiv Audience Analysis
1. This title will update as analysis is completed
2. This section will describe how the above process is conducted
