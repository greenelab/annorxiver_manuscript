## Methods

### Datasets

#### BioRxiv
BioRxiv [@doi:10.1101/833400] is a repository of biological and biomedical research preprints.
We downloaded an xml snapshot of this repository on February 3, 2020 from   bioRxiv's Amazon S3 resource [@https://www.biorxiv.org/tdm] that contained the full text and image content of 98,023 preprints.
Preprints on bioRxiv are versioned, and in our snapshot 26,905 of 98,023 contained more than one version.
When preprints had multiple versions, we used only the latest one. 
Preprints in this snapshot were grouped into one of twenty-nine different categories.
Each preprint was also classified as a new result, confirmatory finding, or contradictory finding.
Some preprints in this snapshot have been withdrawn from bioRxiv.
When a preprint is withdrawn, its content is replaced with the reason for withdrawal.
Because we used the latest version, withdrawn preprints in our analysis contained only statements indicating their removal.

#### PubMed Central
PubMed Central (PMC) [@doi:10.1073/pnas.98.2.381] is a repository that contains free-to-read articles.
PMC contains two types of contributions: closed access articles from research funded by the United States National Institutes of Health (NIH) appearing after an embargo period and articles published under Gold Open Access [@doi:10.1007/s12471-017-1064-2] publishing schemes.
Paper availability within PMC is largely dependent on the journal's participation level [@url:https://www.ncbi.nlm.nih.gov/pmc/about/submission-methods/].
Individual journals have can fully participate in submitting articles to PMC, selectively participate sending only a few few of papers to PMC, only submit papers according to NIH's public access policy [@url:https://grants.nih.gov/grants/policy/nihgps/html5/section_8/8.2.2_nih_public_access_policy.htm], or not participate at all.
As of September 2019, PMC had 5,725,819 articles available [@url:https://www.ncbi.nlm.nih.gov/pmc/about/intro/].
Out of these 5 million articles, about 3 million were open access and available for text processing systems [@doi:10.1093/bioinformatics/btz070;@doi:10.1093/nar/gkz389].
We downloaded a snapshot of this open access subset on January 31, 2020.
This snapshot contains papers such as literature reviews, book reviews, editorials, case reports, research articles and more; however, we used only the research articles.

### Comparing Corpora
We used gensim [@raw:rehurek_lrec] (version 3.8.1) to preprocess the bioRxiv and PubMed Central corpora.
We removed the 337 gensim-provided stopwords.
Throughout our analysis we encountered non-word symbols (e.g., $\pm$), so we refer to words and symbols as tokens to avoid confusion.

Following the cleaning process, we calculated the frequency of every token shared between both corpora.
Because many tokens were unique to one set or the other and observed at low frequency, we used the union of the top 100 most frequent tokens from each corpus to compare them.
We generated a contingency table and calculated the odds ratio for each token.
Furthermore, we also calculated the 95% confidence interval for each odds ratio [@https://www.ncbi.nlm.nih.gov/books/NBK431098/].

### Visualizing the Preprint Landscape

#### Generate Document Representation
We used gensim [@raw:rehurek_lrec] (version 3.8.1) to train a word2vec continuous bag of words (CBOW) [@arxiv:1301.3781] model over the bioRxiv corpus. 
Our neural network architecture had 300 hidden nodes, and we trained this model for 20 epochs.
We set a fixed random seed and otherwise used gensim's default settings.
Following training, we generated a document vector for every article within bioRxiv and PubMed Central.
This document vector is calculated by taking the average of every token present within a given article, ignoring those that were absent from the word2vec model.

#### Dimensionality Reduction of Document Embeddings
We used principal component analysis (PCA) [@doi:10.1111/1467-9868.00196] to project bioRxiv document vectors into a low dimensional space.
We trained this model using scikit-learn's [@raw:scikit-learn] implementation of a randomized solver [@arxiv:0909.4061] with a random seed of 100, output of 50 principal components, and default settings for all other parameters.
For each principal component we calculated its cosine similarity with  all tokens in our word2vec model's vocabulary.
We report the top 100 positive and negative scoring tokens in the form of  word clouds, where the size of each word corresponds to the magnitude of similarity and color represents positive (blue) or negative (orange) association.

### Discovering Unannotated Preprint-Publication Relationships

Automated procedures are in place to link preprints to peer reviewed versions and many journals require authors to update preprints with a link to the published version.
However, automated procedures at _bioRxiv_ are often based on exact matching of certain attributes and authors can forget to establish a link after publication.
For example, authors can change the title between a preprint and published version (e.g., [@doi:10.1101/376665] and [@doi:10.1242/bio.038232]), which prevents _bioRxiv_ from automatically establishing a link.
If the authors do not report the publication to _bioRxiv_, the preprint and the published version are treated as distinct entities despite representing the same underlying research. 
We recognized that the distance in embedding space could reveal preprint to published version links that were missed by existing automated processes.
First, we used CrossRef [@doi:10.1629/uksg.233] to identify bioRxiv preprints that were linked to a corresponding published article.
We filtered out links that contained papers not in PubMed Central's Open Access corpus.
Following the preprocessing step, we calculated the distribution of known preprint to published distances by taking the Euclidean distance between the preprint's embedding coordinates and the coordinates of the published version.
We also calculated a background distribution, which consisted of the distance between each preprint with an annotated publication and a randomly selected article from the same journal the published version.
Next, we calculated distances between preprints without a published version link with PubMed Central articles that weren't matched with a corresponding preprint.
We filtered any potential links with distances that were greater than the minimum value of the background distribution to reduce the curation burden.
Lastly, we binned the remaining pairs based on percentiles from the annotated pairs at the [0,25th percentile), [25th percentile, 50th percentile), [50th percentile, 75th percentile), and [75th percentile, minimum background distance).
We randomly sampled 50 articles from each bin for manual annotation.
We shuffled these four sets to produce a list of 200 potential preprint-published pairs with a randomized order.
We supplied these candidates to two scientists to manually determine if each link between a preprint and a putative matched version was correct or incorrect.
After the curation process, we encountered eight disagreements between reviewers.
These disagreements were supplied to a third scientist, who carefully reviewed each case and made a final determination.
Lastly, we used this curated set to evaluate the extent to which distance in the embedding space revealed true but unannotated links between preprints and their published verisons.

### Journal Recommendation

We aimed to predict the journal a paper would eventually be published in based on its embedding representation.
We illustrate our recommendations as a short list along with a network visualization available at [https://greenelab.github.io/annorxiver-journal-recommender/](https://greenelab.github.io/annorxiver-journal-recommender/).
Since we sought to examine if embeddings were related to publication venue, we used a simple k-nearest neighbors approach with Euclidean distance to recommend journals.

First, we filtered all journals that had fewer than 100 papers in the PMC dataset.
A subset of our PMC corpus was directly linked to papers in bioRxiv as they had been published as open access articles.
We held out this subset and treated it as our gold standard test set.
We used the remainder of the PMC corpus for training and initial evaluation via cross validation.
We considered a list of ten journal suggestions to be an appropriate number and we considered a prediction to be a true positive if the correct journal appeared within the ten closest neighbors of the query article.

Certain journals publish articles in a focused topic area, while others publish articles that cover many topics.
Likewise, some journals have a publication rate of at most hundreds of papers per year while others publish at a rate of at least ten-thousand papers per year.
Accounting for these characteristics, we designed two approaches for recommending journals.

The first approach is based on individual paper proximity, which enabled us to provide an example of the specific article or articles that led to the prediction.
Conversely, predictions using this technique could be biased due to the overrepresentation of general topic journals.
We call this approach the paper-based classifier.
This classifier takes a query article that has been projected onto the embedding space trained on bioRxiv preprints as input and reports the journals of the top ten closest papers.
The number of journals returned via this method could be less than ten as multiple papers in close proximity to query article may belong to the same journal. 

The second approach is based on close proximity to a journal's centroid.
This technique provides recommendations that are more focused on domain-specific publication venues.
We call this approach the journal-based classifier.
This classifier was trained by computing journal centroids via taking the average embedding of all papers published in each journal.
Following the centroid calculation, this classifier takes a query article projected onto the same embedding space as above for input and reports the top ten nearest journals centroids.
Both the paper-based classifier and the journal-based classifier were optimized via 10-fold cross validation.
Lastly, we evaluated performance of both classifiers on our gold standard test set of published preprints.
